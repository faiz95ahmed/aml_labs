{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Demonstration\n",
    "\n",
    "Once you have finished implementing your autograd engine, and it passes the tests, let's see some things that even this simple version of autograd let's you do."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from engine import Value\n",
    "import random\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# approximate samples from N(0, 1)\n",
    "def randn():\n",
    "    return sum([random.random() for _ in range(12)]) - 6\n",
    "\n",
    "beta_t = 0.8\n",
    "sigma = 0.1\n",
    "\n",
    "x = [randn() for _ in range(40)]\n",
    "y = [beta_t * x + sigma * randn() for x in x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear regression redux\n",
    "\n",
    "Using your implemented autograd library, you should be able to find the parameter beta using gradient descent by minimising the sum of squares. I've left some of this for you to fill in the gaps to make a complete gradient descent routine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "beta = Value(0.0)\n",
    "\n",
    "def cost_function(beta):\n",
    "    return sum([(beta * x_i - y_i) ** 2 for (x_i, y_i) in zip(x, y)])\n",
    "\n",
    "\n",
    "loss_history = []\n",
    "while True:\n",
    "    \n",
    "    # FILL IN HERE\n",
    "    loss = cost_function(beta)\n",
    "    loss.backward()\n",
    "    g = beta.grad\n",
    "    \n",
    "    # FILL IN HERE\n",
    "    \n",
    "    loss_history.append(loss.data)\n",
    "    # test for convergence since this is a convex function? - is grad(beta) == 0? \n",
    "    if abs(beta.grad) < 1e-6:\n",
    "        print('converged')\n",
    "        break\n",
    "    \n",
    "print(beta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(loss_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's have a look at how we can use this to build a (very inefficient!) neural network library. \n",
    "\n",
    "Here are some simple neural network building blocks, written in terms of the Value class that you have been working on in this tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Module:\n",
    "\n",
    "    def zero_grad(self):\n",
    "        for p in self.parameters():\n",
    "            p.grad = 0\n",
    "\n",
    "    def parameters(self):\n",
    "        return []\n",
    "\n",
    "class Neuron(Module):\n",
    "\n",
    "    def __init__(self, nin, nonlin=True):\n",
    "        self.w = [Value(random.uniform(-1,1)) for _ in range(nin)]\n",
    "        self.b = Value(0)\n",
    "        self.nonlin = nonlin\n",
    "\n",
    "    def __call__(self, x):\n",
    "        act = sum((wi*xi for wi,xi in zip(self.w, x)), self.b)\n",
    "        return act.relu() if self.nonlin else act\n",
    "\n",
    "    def parameters(self):\n",
    "        return self.w + [self.b]\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"{'ReLU' if self.nonlin else 'Linear'}Neuron({len(self.w)})\"\n",
    "\n",
    "class Layer(Module):\n",
    "\n",
    "    def __init__(self, nin, nout, **kwargs):\n",
    "        self.neurons = [Neuron(nin, **kwargs) for _ in range(nout)]\n",
    "\n",
    "    def __call__(self, x):\n",
    "        out = [n(x) for n in self.neurons]\n",
    "        return out[0] if len(out) == 1 else out\n",
    "\n",
    "    def parameters(self):\n",
    "        return [p for n in self.neurons for p in n.parameters()]\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"Layer of [{', '.join(str(n) for n in self.neurons)}]\"\n",
    "\n",
    "class MLP(Module):\n",
    "\n",
    "    def __init__(self, nin, nouts):\n",
    "        sz = [nin] + nouts\n",
    "        self.layers = [Layer(sz[i], sz[i+1], nonlin=i!=len(nouts)-1) for i in range(len(nouts))]\n",
    "\n",
    "    def __call__(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x.sigmoid()\n",
    "\n",
    "    def parameters(self):\n",
    "        return [p for layer in self.layers for p in layer.parameters()]\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"MLP of [{', '.join(str(layer) for layer in self.layers)}]\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and here's some silly fake data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.datasets import make_moons, make_blobs\n",
    "X, Y = make_moons(n_samples=100, noise=0.1)\n",
    "\n",
    "# visualize in 2D\n",
    "plt.figure(figsize=(5,5))\n",
    "plt.scatter(X[:,0], X[:,1], c=Y, s=20, cmap='jet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = MLP(2, [16, 16, 1]) # this has to be tiny because this scalar-valued tracking is very inefficient.\n",
    "net([0.3, 0.3])\n",
    "\n",
    "losses = []\n",
    "for e in range(25):\n",
    "    correct = 0\n",
    "    for i in range(X.shape[0]):\n",
    "        x = list(X[i])\n",
    "        y = Y[i]\n",
    "\n",
    "\n",
    "        net.zero_grad()\n",
    "        y_pred = net(x)\n",
    "\n",
    "        # very simple mean squared error loss\n",
    "        loss = (y_pred - y) ** 2\n",
    "        loss.backward()\n",
    "\n",
    "        correct += round(y_pred.data) == y\n",
    "        for p in net.parameters():\n",
    "            p.data = p.data - 1e-1 * p.grad\n",
    "        losses.append(loss.data)\n",
    "    print('finished epoch ', e, 'accuracy', correct / X.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "h = 0.25\n",
    "x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
    "                     np.arange(y_min, y_max, h))\n",
    "Xmesh = np.c_[xx.ravel(), yy.ravel()]\n",
    "inputs = [list(map(Value, xrow)) for xrow in Xmesh]\n",
    "scores = list(map(net, inputs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "Z = np.array([s.data for s in scores])\n",
    "Z = Z.reshape(xx.shape)\n",
    "plt.contourf(xx, yy, Z, cmap=plt.cm.Spectral, alpha=0.8)\n",
    "plt.scatter(X[:, 0], X[:, 1], c=Y, s=40, cmap=plt.cm.Spectral)\n",
    "plt.xlim(xx.min(), xx.max())\n",
    "plt.ylim(yy.min(), yy.max())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because we can express complex functions in terms of a small number of primitives, once we have implemented these primitives we can differentiate arbitrarily complex compositions of them by applying the chain rule over and over again!\n",
    "\n",
    "Of course, it won't always be the most efficient way to do it, and in practice a 'real' autograd library would take advantage of vector processing, like numpy. But at it's heart it's a really simple and easy idea!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Possible extension if you finish\n",
    "\n",
    "We aren't limited to neural networks here - anything that can be expressed as a composition of ordinary arithmetic is attackable with autograd.\n",
    "\n",
    "Here is some code to simulate the equations of motion of a [double pendulum](https://en.wikipedia.org/wiki/Double_pendulum), a physical system which displays chaotic behaviour and has no closed form solutions, using Eulers method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dx/dt = f(x)\n",
    "# the pendulum stata is the angle of the two arms, and the corresponding angular momenta\n",
    "def compound_pendulum_velocity(state):\n",
    "    theta_1, theta_2, p_1, p_2 = state\n",
    "    theta_1_dot = 6 * ((2 * p_1 - 3 * (theta_1 - theta_2).cos() * p_2) \n",
    "                   / (16 - 9 * (theta_1 - theta_2).cos() ** 2))\n",
    "    theta_2_dot = 6 * ((8 * p_2 - 3 * (theta_1 - theta_2).cos() * p_1) \n",
    "                   / (16 - 9 * (theta_1 - theta_2).cos() ** 2))\n",
    "    p_1_dot = - 0.5 * (theta_1_dot * theta_2_dot * (theta_1 - theta_2).sin()\n",
    "                       + 3 * 9.8 * theta_1.sin())\n",
    "    p_2_dot = - 0.5 * (-theta_1_dot * theta_2_dot * (theta_1 - theta_2).sin() + 9.8 * theta_2.sin())\n",
    "    return [theta_1_dot, theta_2_dot, p_1_dot, p_2_dot]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def euler(s_0, f, h=1e-3, steps=500):\n",
    "    history = [s_0]\n",
    "    s = s_0\n",
    "    for i in range(steps):\n",
    "        s = [x + h * v for (x, v) in zip(s, f(s))]\n",
    "        history.append(s)\n",
    "    return history\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "                #    theta_1,     theta_2,    p_1,        p_2\n",
    "initial_conditions = [Value(0.4), Value(0.0), Value(1.0), Value(4.0)]\n",
    "dt = 1e-2\n",
    "traj = euler(initial_conditions,\n",
    "             compound_pendulum_velocity,\n",
    "             h=dt,\n",
    "            steps=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t1, t2, _, _ = zip(*traj)\n",
    "# convert to numpy to visualise\n",
    "\n",
    "t1 = np.array([x.data for x in t1])\n",
    "t2 = np.array([x.data for x in t2])\n",
    "\n",
    "x1 = np.sin(t1)\n",
    "y1 = -np.cos(t1)\n",
    "x2 = x1 +  np.sin(t2)\n",
    "y2 = y1 + - np.cos(t2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.animation as animation\n",
    "from matplotlib import pyplot as plt\n",
    "from IPython.display import HTML\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111, autoscale_on=False, xlim=(-2, 2), ylim=(-2, 2))\n",
    "ax.set_aspect('equal')\n",
    "ax.grid()\n",
    "\n",
    "line, = ax.plot([], [], 'o-', lw=2)\n",
    "time_template = 'time = %.1fs'\n",
    "time_text = ax.text(0.05, 0.9, '', transform=ax.transAxes)\n",
    "dt = 0.01\n",
    "def init():\n",
    "    line.set_data([], [])\n",
    "    time_text.set_text('')\n",
    "    return line, time_text\n",
    "\n",
    "\n",
    "def animate(i):\n",
    "    thisx = [0, x1[i], x2[i]]\n",
    "    thisy = [0, y1[i], y2[i]]\n",
    "\n",
    "    line.set_data(thisx, thisy)\n",
    "    time_text.set_text(time_template % (i*dt))\n",
    "    return line, time_text\n",
    "\n",
    "\n",
    "ani = animation.FuncAnimation(fig, animate, range(1, len(traj)),\n",
    "                              interval=dt*1000, blit=True, init_func=init)\n",
    "HTML(ani.to_html5_video())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But look - because we implemented this on top of Autograd, we can differentiate through our ODE integration routine, and get the gradient of the final positions with respect to our initial conditions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "traj[-1][1].backward()\n",
    "initial_conditions[0].grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## some things to play with?\n",
    "\n",
    "The gradient is a measure of first-order sensitivity to initial conditions. Can you find initial conditions where the final position is particularly sensitive to this? What does that mean about a perturbation to the system? Can you predict initial conditions that exhibit particularly stable or chaotic behaviour using your numerically calculated gradients?\n",
    "\n",
    "The double pendulum has an unstable equilibrium point when the pendulum is upside down, where $\\theta_1 = \\theta_2 = \\pi$. Can you use autograd to optimise the initial momentum so that the pendulum balances in this state for as long as possible?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
