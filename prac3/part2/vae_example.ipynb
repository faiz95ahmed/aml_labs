{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic VAE implementation\n",
    "\n",
    "This notebook defines a basic VAE to model the MNIST dataset, using PyTorch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VAE Theory\n",
    "\n",
    "The goal of a VAE is to (implicitly) learn a distribution $p(x),\\,x \\in X$, with $X$ a vector space, which is as similar as possible to an unknown target distribution $p^{*}(x)$, given a set of samples from this distribution $\\mathcal{D}=\\{x_1, ..., x_{n}\\}$. VAEs make the assumption that $x$ depends on some set of underlying latent variables $z \\in Z$, which are distributed according to some known distribution $p(z)$, and model the likelihood $p(x\\mid z)$ and an approximation of the posterior distribution $p(z \\mid x)$.\n",
    "\n",
    "To help them learn, VAEs use variational inference. In this approach, instead of computing the posterior $p(z \\mid x)$, which is often intractable, one searches for a more tractable distribution $q(z \\mid x)$ which approximates the posterior. VAEs assume parameterised models of the likelihood and the approximate posterior, given by $p(x \\mid z,\\theta)$ and $q(z \\mid x, \\theta)$, where $\\theta$ represent model parameters. To learn  $q(z \\mid x, \\theta)$, we maximise the Evidence Lower Bound (or ELBO) over the training data, which is given by\n",
    "\n",
    "\\begin{equation}\n",
    "ELBO = \\sum_{i=1}^{n} \\mathbb{E}_{q(z_{i} \\mid x_{i},\\theta)} \\left[\\log p(x_{i} \\mid z_{i},\\theta) \\right] - D_{\\mathrm{KL}}(q(z_{i}\\mid x_{i},\\theta) \\, \\| \\, p(z_{i}))~,\n",
    "\\end{equation}\n",
    "assuming the training samples are independently and identically distributed. \n",
    "\n",
    "To be able to compute the ELBO, we assume a Gaussian VAE:\n",
    "\\begin{align}\n",
    "\\label{eq:vae_likelihood}\n",
    "p(x \\mid z,\\theta)&=\\mathcal{N}(x; f(z; \\theta_{f}), \\sigma^{2} I)\n",
    "\\\\\n",
    "\\label{eq:vae_prior}\n",
    "p(z)&=\\mathcal{N}(z; 0, I)\n",
    "\\\\\n",
    "\\label{eq:vae_q}\n",
    "q(z\\mid x,\\theta)&=\\mathcal{N}(z; g(x;\\theta_{g}), \\mathrm{diag}(h(x;\\theta_{h})^{2}))~,\n",
    "\\end{align}\n",
    "where $f$, $g$ and $h$ are all deep neural networks parameterised by $\\theta=\\{\\theta_{f},\\theta_{g},\\theta_{h}\\}$, $\\sigma$ is a hyperparameter. Our optimal network parameters are then given by\n",
    "\n",
    "\\begin{equation}\n",
    "\\label{eq:vae_mle}\n",
    "\\hat \\theta_{f}, \\hat \\theta_{g}, \\hat \\theta_{h} = argmin_{\\theta_{f}, \\theta_{g}, \\theta_{h}}  \\left\\{ \\sum_{i=1}^{n} \\mathbb{E}_{q(z_{i} \\mid x_{i}, \\theta_{g},\\theta_{h})} \\left[{\\| x_{i}-f(z_{i};\\theta_{f}) \\|^{2} \\over 2 \\sigma^{2}}\\right] + D_{\\mathrm{KL}}(q(z_{i}\\mid x_{i},\\theta_{g},\\theta_{h}) \\, \\| \\, p(z_{i})) \\right\\}~.\n",
    "\\end{equation}\n",
    "\n",
    "The KL term above can easily be computed analytically as it involves two normal distributions. When calculating the expectation over $q(z\\mid x,\\theta)$, which is usually carried out via sampling, a \"reparameterisation trick\" is used. One notes that sampling $z \\sim q(z\\mid x,\\theta)$ is equivalent to sampling $\\epsilon \\sim \\mathcal{N}(\\epsilon;0,I)$ and computing $z=h(x;\\theta_{h})\\epsilon + g(x;\\theta_{g})$, which allows gradient descent methods to be used to find $\\hat \\theta$.\n",
    "\n",
    "> **Extension task**: show that the KL divergence term in this case can be analytically derived as: $D_{\\mathrm{KL}}(q(z_{i}\\mid x_{i},\\theta_{g},\\theta_{h}) \\, \\| \\, p(z_{i}))= -{1\\over 2} \\sum^{J}_{j=1} \\left[ 1 + \\log(h_{j}^{2}) - h_{j}^{2} - g_{j}^{2} \\right]$ where $J$ is the dimension of the latent vector.\n",
    "\n",
    "For more on VAE theory, see: https://arxiv.org/abs/1606.05908 and https://arxiv.org/abs/1907.08956."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load training data\n",
    "\n",
    "First, we load the MNIST training data (similar to before), and plot some training examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_dataset = datasets.MNIST('./data', train=True, download=True,\n",
    "                   transform=transforms.Compose([\n",
    "                       transforms.ToTensor(),\n",
    "                   ]))\n",
    "\n",
    "test_dataset = datasets.MNIST('./data', train=False, download=True,\n",
    "                   transform=transforms.Compose([\n",
    "                       transforms.ToTensor(),\n",
    "                   ]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(train_dataset))\n",
    "print(len(test_dataset))\n",
    "\n",
    "for i in [0, 101]:\n",
    "    im,label = train_dataset[i]\n",
    "    \n",
    "    plt.imshow(im.numpy()[0])\n",
    "    plt.title(label)\n",
    "    plt.colorbar()\n",
    "    plt.show()\n",
    "\n",
    "print(im.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define network\n",
    "\n",
    "Next we define the VAE."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Task 1**: comple the PyTorch VAE model below by implementing the `reparameterize` method which samples from the approximate posterior using the reparameterisation trick, and implementing the model's `forward` method, which returns the estimated parameters $f$,$g$ and $h$ of the likelihood and approximate posterior given a sample $x$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class VAE(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(VAE, self).__init__()\n",
    "\n",
    "        self.fc1 = nn.Linear(784, 400)\n",
    "        self.fc21 = nn.Linear(400, 20)\n",
    "        self.fc22 = nn.Linear(400, 20)\n",
    "        self.fc3 = nn.Linear(20, 400)\n",
    "        self.fc4 = nn.Linear(400, 784)\n",
    "\n",
    "    def encode(self, x):\n",
    "        h1 = F.relu(self.fc1(x))\n",
    "        return self.fc21(h1), self.fc22(h1)# estimates mu, log-variance of approximate posterior q(z|x)\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        \n",
    "        # TODO: write some code which returns a random sample from the approximate posterior q(z|x) here\n",
    "        \n",
    "\n",
    "\n",
    "    def decode(self, z):\n",
    "        h3 = F.relu(self.fc3(z))\n",
    "        return torch.sigmoid(self.fc4(h3))# estimates mu of likelihood p(x|z)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        # TODO: write some code which returns\n",
    "        # 1) mu,log-variance of the approximate posterior given x\n",
    "        # 2) mu of the likelihood given a random sample from the posterior given x\n",
    "        \n",
    "\n",
    "        \n",
    "        return recon, mu, logvar# should return  (likelihood mu, posterior mu, posterior log-variance)\n",
    "    \n",
    "model = VAE()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss function\n",
    "\n",
    "Next, we define the loss function to train the VAE model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Task 2**: define a loss function to train the VAE model, which minimises the expression for the negative ELBO shown above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_function(recon, x, mu, logvar):\n",
    "    \n",
    "    # TODO: write some code here to compute the negative ELBO shown above.\n",
    "    \n",
    "    # The reconstruction term can be approximated with a single sample from the approximate posterior.\n",
    "    # Use the analytical exppression for the KL term.\n",
    "    # recon, mu and logvar are the outputs from model.forward(x).\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train\n",
    "\n",
    "Finally, we train the VAE model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import optim\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)# this automatically batches up examples, adding a batch dimension\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Task 3**: write some code to train the VAE using the training data and the loss function defined above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch):\n",
    "    model.train()\n",
    "    \n",
    "    train_loss = 0\n",
    "    for batch_idx, (data, _) in enumerate(train_loader):\n",
    "        \n",
    "        # TODO: write some code here\n",
    "        # carry out an optimisation step for each batch\n",
    "        # after each epoch report the average loss over the training dataset\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "def test(epoch):\n",
    "    model.eval()\n",
    "    \n",
    "    test_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for i, (data, _) in enumerate(test_loader):\n",
    "            \n",
    "            # TODO: write some code here\n",
    "            # report the average loss over the test dataset\n",
    "            \n",
    "\n",
    "\n",
    "for epoch in range(1, 5):\n",
    "    train(epoch)\n",
    "    test(epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate\n",
    "\n",
    "In this section we evaluate the performance of the trained VAE.\n",
    "\n",
    "### Reconstruction quality\n",
    "\n",
    "> **Task 4**: plot example VAE reconstructions (mean of the likelihood) for some of the images in the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: write some code here\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Latent space interpolation\n",
    "\n",
    "Now that we have learnt a latent representation, we can interpolate smoothly between two digits by linearly interpolating between them in the latent space.\n",
    "\n",
    "> **Task 5**: show the mean of the likelihood (image reconstruction) for a set latent vectors which are linearly interpolated between two test images in the latent space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# TODO: write some code here\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random sampling of the posterior\n",
    "\n",
    "We can also generate random samples of $x$ given values of $z$ drawn from its prior distribution (a unit Gaussian).\n",
    "\n",
    "> **Task 6**: generate random latent vectors from the prior latent distribution and show the resulting likelihood means (image reconstructions)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# TODO: write some code here\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extensions\n",
    "\n",
    "Have a go at these if you have time.\n",
    "\n",
    "> What happens to the performance when you reduce the dimensionality of the latent space?\n",
    "\n",
    "> Try using a Beta-VAE (https://openreview.net/forum?id=Sy2fzU9gl) : what happens when the KL term is made much stronger?\n",
    "\n",
    "> Try using a binary cross-entropy loss instead of a L2 loss in the loss function (e.g. https://github.com/pytorch/examples/blob/master/vae/main.py)\n",
    "\n",
    "> Try to run clustering in the latent space to label the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
